---
layout : post
section-type: post
title : Adventures in Data Hunting
author : Jay
---

Looking for data on the internet is like playing Pokemon Go- You aim to find as many as you can; there is an abundance of them in easy to access places, but the ones found there are either underwhelming or too common; for the more exotic and powerful ones you have to travel far and beyond. Even when you find one, there is no guarantee that it would be caught easily, it might be evasive, ill-behaved and might need to be knocked down by your tools, which depends on your skill and acumen. After catching you need to train it regularly, earn its respect and become good buddies in order to understand its deeply.
In the end, only if you have the grit and character to tame it, can you acheive eternal glory. This post is one of (hopefully) many to come of my data hunting adventures.

For my next project, I was looking for some data closer to my heart, something I could relate to. So, I browsed to data.gov.in, the Indian equivalent of data.gov to look for datasets on my country. After recently getting back to working out, all my mind could think of was health and fitness, which is what I looked for.  Sadly, all the data on health was either scattered or old, none newer than 2005-06. I took it upon myself to leave no stone unturned in pursuit of the data. I found the location of the data fairly easily, it lay on the website of the <a href="http://rchiips.org/nfhs/"> National Family Health Survey </a>, all bright and shining, divided by States and even districts (Indian equivalent of US Counties) for the year 2015-2016. As the name suggests, the NFHS is a survey of Indian households on key health indicators. The data contains the percentage of people that fulfill certain criterion. But there remained significant obstacles between me and the form of data I could potentially use for analysis.

For starters, the data was split into several bins- one for each state- **and one pdf file per district**. I would have to write a scraper to first pull all the files before I would run into the landmine of pdf parsing. So, I did what any self-respecting data hunter would do- lower my goals. I decided to focus only on my state of residence- Madhya Pradesh- instead of jumping into python and writing a scraper for all 28 states and their thousand odd districts. The task was manual, but not as painstaking, to click on each link and download the file. Five minutes later, I had a folder of fifty pdfs, one for each district.

Anyone who has done data analysis knows that pdf files are only two steps better than having no data (the other step being physical, printed data). Many times, old scanned reports are stored as PDFs, for which there is no alternative other than have to manually enter the data in a different format. Thankfully, these PDFs were digitally created and hence had a defined structure. A big shoutout to the people at <a href="tabula.technology"> Tabula </a> and the creator of it's <a href= "https://github.com/chezou/tabula-py"> python wrapper </a> which made the pdf parsing drastically more simple. All in all the data was well behaved. The file structure was consistent and there was little missing data. More so, even the page numbers of the pages containing tables were same for all pdfs.

But not all was hunky-dory. Instead of boring you with each line of python script (which you can find it on my <a href="https://github.com/jparchure/datasets"> github </a>, under the data folder and then scripts), I will talk about some of the challenges I faced while parsing the files. For starters, the table structure and headings were not consistent. Some of the districts were rural hence their column heading read "Rural", whereas others were urban. I missed this while picking the sample of pdfs to parse from and had to accommodate this division my code. But this was not all, there existed some districts which were classified as both rural and urban, thus altering the structure of the table and the resulting csv. This taught me the valuable lesson of not jumping to code before manually inspecting a significantly diverse sample of data. Next, the features or the health indicators were inconsistent, as in some of them were one item per row whereas some spanned two based on their length. This caused Tabula to misbehave and I had to manually fix those seven indicators.

But when all was done, the final product came out to be really well. Since it was automated, I can easily extend it to different states by adding a scraper which extracts file of the National Health Survey website. It was my first experience in data gathering from scratch to finish. You can find a copy of the final csv on my github.

Going forward, I hope to add more states by either creating more files or combining all in one single csv file. I also plan to hunt for more data from different domains and discuss my approach in later posts. Should you ever use this data or the scripts, please drop me an email with your analysis as an FYI.
